{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define paths\n",
    "base_dir = 'content/Diabetic_Balanced_Data'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "val_dir = os.path.join(base_dir, 'val')\n",
    "test_dir = os.path.join(base_dir, 'test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image size and batch size\n",
    "img_height, img_width = 224, 224\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform images for training, validating and testing \n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((img_height, img_width)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((img_height, img_width)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((img_height, img_width)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_datasets():\n",
    "    print(\"Loading datasets...\")\n",
    "    train_dataset = datasets.ImageFolder(train_dir, data_transforms['train'])\n",
    "    val_dataset = datasets.ImageFolder(val_dir, data_transforms['val'])\n",
    "    test_dataset = datasets.ImageFolder(test_dir, data_transforms['test'])\n",
    "    \n",
    "    # Create data loaders\n",
    "    #num_workers=0 to avoid multiprocessing issues - no clue what that is  - check \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    \n",
    "    #printing inf o \n",
    "    classes = train_dataset.classes\n",
    "    print(\"\\n--- Dataset Information ---\")\n",
    "    print(f\"Training set size: {len(train_dataset)} images\")\n",
    "    print(f\"Validation set size: {len(val_dataset)} images\")\n",
    "    print(f\"Test set size: {len(test_dataset)} images\")\n",
    "    print(f\"Classes: {classes}\")\n",
    "    print(\"---------------------------\\n\")\n",
    "    \n",
    "    \n",
    "    class_counts = {cls: 0 for cls in classes}\n",
    "    for _, label in train_dataset.samples:\n",
    "        class_counts[classes[label]] += 1\n",
    "    print(\"Class distribution in training set:\")\n",
    "    for cls, count in class_counts.items():\n",
    "        print(f\"{cls}: {count} images\")\n",
    "    print()\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#keeping track of training \n",
    "def plot_training_history(history, save_path=None):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot training and validation accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_acc'], label='Train Accuracy')\n",
    "    plt.plot(history['val_acc'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot training and validation loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"Training history plot saved to {save_path}\")\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training function with detailed progress tracking\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=30, phase=\"initial_training\", save_path=None):\n",
    "    best_val_acc = 0.0\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    total_start_time = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\n[{phase}] Epoch {epoch+1}/{num_epochs}\")\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        \n",
    "        # Progress bar for training\n",
    "        train_bar = tqdm(train_loader, desc=f\"Training\")\n",
    "        for inputs, labels in train_bar:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            batch_loss = loss.item() * inputs.size(0)\n",
    "            batch_corrects = torch.sum(preds == labels.data).double()\n",
    "            \n",
    "            running_loss += batch_loss\n",
    "            running_corrects += batch_corrects\n",
    "            \n",
    "            # Update progress bar with current batch loss and accuracy\n",
    "            batch_acc = batch_corrects / inputs.size(0)\n",
    "            train_bar.set_postfix({\n",
    "                'loss': f\"{loss.item():.4f}\",\n",
    "                'acc': f\"{batch_acc:.4f}\"\n",
    "            })\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = running_corrects / len(train_loader.dataset)\n",
    "        \n",
    "        # Store training metrics\n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        history['train_acc'].append(epoch_acc.item())\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_corrects = 0\n",
    "        \n",
    "        # Progress bar for validation\n",
    "        val_bar = tqdm(val_loader, desc=f\"Validation\")\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_bar:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                batch_loss = loss.item() * inputs.size(0)\n",
    "                batch_corrects = torch.sum(preds == labels.data).double()\n",
    "                \n",
    "                val_loss += batch_loss\n",
    "                val_corrects += batch_corrects\n",
    "                \n",
    "                # Update progress bar\n",
    "                batch_acc = batch_corrects / inputs.size(0)\n",
    "                val_bar.set_postfix({\n",
    "                    'loss': f\"{loss.item():.4f}\",\n",
    "                    'acc': f\"{batch_acc:.4f}\"\n",
    "                })\n",
    "        \n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_acc = val_corrects / len(val_loader.dataset)\n",
    "        \n",
    "        # Store validation metrics\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc.item())\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f\"\\n[{phase}] Epoch {epoch+1}/{num_epochs} Summary:\")\n",
    "        print(f\"  Training   - Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}\")\n",
    "        print(f\"  Validation - Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}\")\n",
    "        print(f\"  Time: {epoch_time:.2f}s\")\n",
    "        \n",
    "        # Save the best model based on validation accuracy\n",
    "        if val_acc > best_val_acc and save_path:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "                'val_acc': val_acc,\n",
    "            }, save_path)\n",
    "            print(f\"  Saved best model with validation accuracy: {val_acc:.4f}\")\n",
    "    \n",
    "    total_time = time.time() - total_start_time\n",
    "    print(f\"\\n[{phase}] Training completed in {total_time/60:.2f} minutes\")\n",
    "    print(f\"Best validation accuracy: {best_val_acc:.4f}\")\n",
    "    \n",
    "    #summary\n",
    "    plot_path = f\"{save_path.split('.')[0]}_history.png\" if save_path else None\n",
    "    plot_training_history(history, save_path=plot_path)\n",
    "    \n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(model, test_loader, criterion, classes):\n",
    "    print(\"\\n=== FINAL MODEL EVALUATION ===\")\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    test_corrects = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    print(\"Evaluating on test set...\")\n",
    "    test_bar = tqdm(test_loader, desc=\"Testing\")\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_bar:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            test_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_acc = test_corrects.double() / len(test_loader.dataset)\n",
    "    \n",
    "    print(\"\\nTest Set Results:\")\n",
    "    print(f\"Loss: {test_loss:.4f}\")\n",
    "    print(f\"Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    # Class-wise accuracy\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=classes))\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    # Load datasets\n",
    "    train_loader, val_loader, test_loader, classes = load_datasets()\n",
    "    \n",
    "    # Check if CUDA is available\n",
    "    global device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load pre-trained EfficientNet-B3 model\n",
    "    print(\"\\nInitializing EfficientNet-B3 model...\")\n",
    "    model = models.efficientnet_b3(weights=\"IMAGENET1K_V1\")  # Updated from pretrained=True\n",
    "    \n",
    "    # Modify the classifier to match the number of classes\n",
    "    num_classes = len(classes)\n",
    "    num_ftrs = model.classifier[1].in_features\n",
    "    model.classifier[1] = nn.Linear(num_ftrs, num_classes)\n",
    "    model = model.to(device)\n",
    "    print(f\"Model output layer modified for {num_classes} classes\")\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # PHASE 1: Initial Training\n",
    "    initial_model_path = 'efficientnet_b3_initial.pth'\n",
    "    print(\"\\n=== STARTING INITIAL TRAINING ===\")\n",
    "    model, initial_history = train_model(model, train_loader, val_loader, criterion, optimizer, \n",
    "                                        num_epochs=30, phase=\"Initial Training\", save_path=initial_model_path)\n",
    "    \n",
    "    print(f\"\\nInitial training completed. Best model saved to {initial_model_path}\")\n",
    "    \n",
    "    # PHASE 2: Fine-tuning\n",
    "    print(\"\\n=== STARTING FINE-TUNING ===\")\n",
    "    print(\"Loading the best initial model...\")\n",
    "    \n",
    "    # Load the saved model\n",
    "    checkpoint = torch.load(initial_model_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Loaded model from epoch {checkpoint['epoch']} with validation accuracy {checkpoint['val_acc']:.4f}\")\n",
    "    \n",
    "    # Freeze most layers and only train the last few layers\n",
    "    print(\"Freezing early layers...\")\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Unfreeze the last few layers for fine-tuning\n",
    "    print(\"Unfreezing the last 10 feature layers and classifier...\")\n",
    "    for param in model.features[-10:].parameters():\n",
    "        param.requires_grad = True\n",
    "    # Also unfreeze the classifier\n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    # Count trainable parameters\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Trainable parameters: {trainable_params:,} ({trainable_params/total_params:.2%} of total)\")\n",
    "    \n",
    "    # Re-define optimizer with a lower learning rate for fine-tuning\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.0001)\n",
    "    print(\"Optimizer reset with learning rate 0.0001\")\n",
    "    \n",
    "    # Fine-tune the model\n",
    "    final_model_path = 'efficientnet_b3_finetuned.pth'\n",
    "    model, finetuning_history = train_model(model, train_loader, val_loader, criterion, optimizer, \n",
    "                                           num_epochs=30, phase=\"Fine-tuning\", save_path=final_model_path)\n",
    "    \n",
    "    print(f\"\\nFine-tuning completed. Best model saved to {final_model_path}\")\n",
    "    \n",
    "    # Load the best fine-tuned model for evaluation\n",
    "    checkpoint = torch.load(final_model_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # Evaluate the model\n",
    "    evaluate_model(model, test_loader, criterion, classes)\n",
    "    \n",
    "    print(\"Training and evaluation completed!\")\n",
    "    print(f\"Initial model saved to: {initial_model_path}\")\n",
    "    print(f\"Fine-tuned model saved to: {final_model_path}\")\n",
    "    print(f\"Training plots and confusion matrix saved to current directory\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
